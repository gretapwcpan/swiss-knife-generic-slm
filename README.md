# Swiss Knife Generic SLM - DistilBERT Implementation

A comprehensive implementation of the **DistilBERT** knowledge distillation methodology for creating smaller language models (SLMs) from large language models (LLMs).

Based on the paper: **"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"** (Sanh et al., 2019)  
üìÑ Paper: [https://arxiv.org/pdf/1909.10351](https://arxiv.org/pdf/1909.10351)

## üéØ Key Features

This implementation faithfully follows the DistilBERT paper methodology:

### 1. **Triple Loss Function**
- **Distillation Loss (Lce)**: KL divergence between teacher and student soft targets
- **Masked Language Modeling Loss (Lmlm)**: Standard BERT MLM objective
- **Cosine Embedding Loss (Lcos)**: Aligns hidden state directions between models

### 2. **Smart Student Initialization**
- Takes every other layer from teacher (e.g., layers 0,2,4,6,8,10 from BERT-base)
- Removes token type embeddings for efficiency
- Initializes weights directly from teacher model

### 3. **Paper-Compliant Training**
- Temperature T=3 for distillation (as per paper)
- Same training data as teacher model
- Learning rate: 5e-4 (paper default)
- Achieves paper's targets: 40% smaller, 60% faster, 97% performance retention

## üìä Expected Results

Following the DistilBERT paper, you should achieve:
- **Size**: 40% reduction in parameters
- **Speed**: 60% faster inference
- **Performance**: 97% of BERT's performance on GLUE tasks
- **Perplexity**: Minimal increase compared to teacher

## üöÄ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/swiss-knife-generic-slm.git
cd swiss-knife-generic-slm

# Install in development mode
pip install -e .
```

## üìñ Quick Start

### 1. Create a DistilBERT from BERT-base

```bash
# Initialize student model (takes layers 0,2,4,6,8,10)
swiss-knife initialize bert-base-uncased --output-dir ./my-distilbert
```

### 2. Train with Knowledge Distillation

```bash
# Distill knowledge using triple loss
swiss-knife distill bert-base-uncased \
    --output-dir ./distilled_model \
    --num-epochs 3 \
    --batch-size 32 \
    --temperature 3.0
```

### 3. Evaluate Performance

```bash
# Compare teacher vs student
swiss-knife evaluate bert-base-uncased ./distilled_model
```

### 4. Benchmark on GLUE

```bash
# Test on GLUE tasks
swiss-knife benchmark ./distilled_model --task sst2
```

## üîß Advanced Usage

### Custom Layer Selection

```python
from swiss_knife import initialize_student_from_teacher

# Select specific layers
student = initialize_student_from_teacher(
    teacher_model="bert-large-uncased",
    student_layers=[0, 3, 6, 9, 12, 15, 18, 21],  # 8 layers from 24
)
```

### Triple Loss Training

```python
from swiss_knife import DistilBERTDistiller

# Initialize distiller with custom weights
distiller = DistilBERTDistiller(
    teacher_model_name="bert-base-uncased",
    temperature=3.0,      # Paper default
    alpha=0.5,           # Distillation loss weight
    beta=2.0,            # MLM loss weight
    gamma=1.0,           # Cosine loss weight
)

# Train on your dataset
metrics = distiller.train(
    train_dataloader=train_loader,
    num_epochs=3,
    learning_rate=5e-4,  # Paper default
)
```

### Evaluation

```python
from swiss_knife import DistillationEvaluator

evaluator = DistillationEvaluator()

# Generate comprehensive report
report = evaluator.generate_evaluation_report(
    teacher_model=teacher,
    student_model=student,
    tokenizer=tokenizer,
    test_texts=test_data,
)
print(report)
```

## üìö Paper Implementation Details

### Loss Functions (Section 3 of paper)

The total loss is a linear combination:
```
L = Œ± * Lce + Œ≤ * Lmlm + Œ≥ * Lcos
```

Where:
- **Lce**: Distillation loss with temperature T=3
- **Lmlm**: Masked language modeling loss (15% masking)
- **Lcos**: Cosine similarity between hidden states

### Student Architecture

Following the paper exactly:
- BERT-base (12 layers) ‚Üí DistilBERT (6 layers)
- Layers selected: [0, 2, 4, 6, 8, 10]
- Token type embeddings removed
- Hidden dimension unchanged (768)

### Training Configuration

Paper specifications:
- **Batch size**: 4K (adjust based on GPU)
- **Learning rate**: 5e-4
- **Optimizer**: Adam
- **Training data**: Same as BERT (Wikipedia + BookCorpus)

## üéì Academic Citation

If you use this implementation, please cite the original DistilBERT paper:

```bibtex
@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}
```

## üìà Benchmarks

| Model | Parameters | Inference Time | GLUE Score | Size |
|-------|------------|----------------|------------|------|
| BERT-base | 110M | 1.0x | 82.1 | 440MB |
| DistilBERT | 66M | 0.6x | 79.8 | 265MB |
| **Reduction** | **40%** | **60% faster** | **97% retained** | **40%** |

## üõ†Ô∏è CLI Commands

```bash
# Show paper information
swiss-knife paper-info

# Show quick start guide
swiss-knife quick-start

# Initialize student from teacher
swiss-knife initialize <teacher_model> [options]

# Train with distillation
swiss-knife distill <teacher_model> [options]

# Evaluate models
swiss-knife evaluate <teacher> <student> [options]

# Benchmark on GLUE
swiss-knife benchmark <model> --task <task_name>
```

## üî¨ Research Applications

This implementation is suitable for:
- Creating efficient models for edge deployment
- Research on knowledge distillation techniques
- Benchmarking compression methods
- Educational purposes (understanding distillation)

## ü§ù Contributing

Contributions are welcome! Please ensure any modifications maintain compatibility with the DistilBERT paper methodology.

## üìÑ License

MIT License - See [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Original DistilBERT paper authors (Sanh et al., 2019)
- Hugging Face Transformers library
- PyTorch team

## ‚ö†Ô∏è Important Notes

1. **GPU Recommended**: Training requires significant computational resources
2. **Memory Requirements**: Adjust batch size based on available GPU memory
3. **Training Time**: Full distillation can take several days on large datasets
4. **Evaluation**: GLUE benchmarking requires task-specific fine-tuning

---

**Paper Reference**: [https://arxiv.org/pdf/1909.10351](https://arxiv.org/pdf/1909.10351)
